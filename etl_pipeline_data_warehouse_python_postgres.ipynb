{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL pipeline and Data warehouse using Python and Postgresql\n",
    "\n",
    "## About This ETL pipeline\n",
    "1. **Extract data** from CSV files i-e product_info.csv and product_reviews.csv. These data belong to an ecommerce website where product_info.csv contains description of products and product_reviews.csv contain the reviews given by customers for that product. The data is loaded from csv files to a dataframe using Pandas library\n",
    "2. **Transform data**. After extracting data from source dataset, the dataset will be transformed to a suitable form. Transformation will include steps like data cleaning, new columns generation etc. Data transformation is done using Pandas library\n",
    "3. **Load data**. Finally load the transformed data into the datawarehouse. Data warehouse is created using Postgresql\n",
    "\n",
    "## About the Datawarehouse\n",
    "Here, in this project, I have created a simple datawarehouse to be stored in postgresql. This datawarehouse follows star schema will a fact table and few dimension tables.  \n",
    "The data model for the datawarehouse I created is shown below:  \n",
    "![Data Warehouse Data Model](diagrams/data_warehouse_schema.jpg)\n",
    "\n",
    "Here **Fact Table:** tbl_product_reviews -> Customer Reviews of products  \n",
    "And **Dimension Tables:**  \n",
    "1. tbl_product: Details of product reviewed\n",
    "2. tbl_brand: Details of Brand on product reviewed\n",
    "3. tbl_reviewer: Details of reviewer who reviewed the product\n",
    "4. tbl_date: Details of date when product was reviewed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For postgresql db connection\n",
    "import psycopg2\n",
    "# for preprocessing\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# for loading data from env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# To check time\n",
    "import time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_path = \"datasets/product_info.csv\"\n",
    "dataset2_path = \"datasets/product_reviews.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "db_host = os.environ.get('DB_HOST')\n",
    "db_user = os.environ.get('DEFAULT_PG_USER')\n",
    "db_pwd = os.environ.get('DEFAULT_PG_PASSWORD')\n",
    "db_default_db = os.environ.get('DEFAULT_DB_NAME')\n",
    "datawarehouse_name = os.environ.get('DATAWAREHOUSE_NAME')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_TABLE_QUERIES = [\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_product(\n",
    "        product_id TEXT PRIMARY KEY,\n",
    "        product_name TEXT,\n",
    "        avg_product_rating FLOAT,\n",
    "        product_price FLOAT,\n",
    "        product_reviews_count FLOAT,\n",
    "        favorites_count INT,\n",
    "        variations_count INT,\n",
    "        product_category TEXT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tbl_brand(\n",
    "        brand_id INT PRIMARY KEY,\n",
    "        brand_name TEXT UNIQUE\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_reviewer(\n",
    "        reviewer_id BIGINT PRIMARY KEY,\n",
    "        reviewer_skin_tone TEXT,\n",
    "        reviewer_skin_type TEXT,\n",
    "        reviewer_eye_color TEXT,\n",
    "        reviewer_hair_color TEXT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_date(\n",
    "        date_id TIMESTAMP PRIMARY KEY,\n",
    "        year INT,\n",
    "        month INT,\n",
    "        day INT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_product_reviews(\n",
    "        review_id INT PRIMARY KEY,\n",
    "        product_id TEXT REFERENCES tbl_product(product_id),\n",
    "        brand_id INT REFERENCES tbl_brand(brand_id),\n",
    "        reviewer_id BIGINT REFERENCES tbl_reviewer(reviewer_id),\n",
    "        date_id TIMESTAMP REFERENCES tbl_date(date_id),\n",
    "        review_title TEXT,\n",
    "        review_text TEXT,\n",
    "        review_rating INT\n",
    "    )\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "DROP_TABLE_QUERIES = [\n",
    "    \"\"\" \n",
    "    DROP TABLE tbl_product_reviews;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    DROP TABLE tbl_product;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    DROP TABLE tbl_brand;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    DROP TABLE tbl_reviewer;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    DROP TABLE tbl_date;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "TRUNCATE_TABLE_QUERIES = [\n",
    "    \"\"\" \n",
    "    TRUNCATE TABLE tbl_product_reviews;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    TRUNCATE TABLE tbl_product CASCADE;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    TRUNCATE TABLE tbl_brand CASCADE;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    TRUNCATE TABLE tbl_reviewer CASCADE;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    TRUNCATE TABLE tbl_date CASCADE;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "INSERT_TABLE_QUERIES = {\n",
    "    \"product\": \n",
    "        \"\"\" \n",
    "        INSERT INTO tbl_product(product_id, product_name, avg_product_rating, product_price, product_reviews_count, favorites_count, variations_count, product_category) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        \"\"\",\n",
    "    \"brand\":\n",
    "        \"\"\" \n",
    "        INSERT INTO tbl_brand(brand_id, brand_name) VALUES (%s,%s)\n",
    "        \"\"\",\n",
    "    \"reviewer\":\n",
    "        \"\"\" \n",
    "        INSERT INTO tbl_reviewer(reviewer_id, reviewer_skin_tone, reviewer_skin_type, reviewer_eye_color, reviewer_hair_color) VALUES (%s,%s,%s,%s,%s)\n",
    "        \"\"\",\n",
    "    \"date\":\n",
    "        \"\"\" \n",
    "        INSERT INTO tbl_date(date_id, year, month, day) VALUES (%s,%s,%s,%s)\n",
    "        \"\"\",\n",
    "    \"reviews\":\n",
    "        \"\"\" \n",
    "        INSERT INTO tbl_product_reviews(review_id, product_id, brand_id, reviewer_id, date_id, review_title, review_text, review_rating) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        \"\"\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will check whether the value is a numeric value or not\n",
    "def checkNumeric(val):\n",
    "    return True if str(val).isdigit() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for creating datawarehouse\n",
    "def create_datawarehouse():\n",
    "    print(\"Started creating data warehouse\")\n",
    "    # connection to connect to default postgresql database\n",
    "    db_conn_string = f\"host={db_host} dbname={db_default_db} user={db_user} password={db_pwd}\"\n",
    "    conn = psycopg2.connect(db_conn_string)\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor() \n",
    "    \n",
    "    # creating the datawarehouse \n",
    "    cur.execute(f\"DROP DATABASE IF EXISTS {datawarehouse_name}\")\n",
    "    cur.execute(f\"CREATE DATABASE {datawarehouse_name}\")\n",
    "    # close connection to default database\n",
    "    print(\"Data warehouse created succesfully\")\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fuctio is used for connecting to the created datawarehouse\n",
    "def connect_datawarehouse():\n",
    "    # creating connection to datawarehouse \n",
    "    dw_string = f\"host={db_host} dbname={datawarehouse_name} user={db_user} password={db_pwd}\"\n",
    "    dw_conn = psycopg2.connect(dw_string)\n",
    "    # creating datawarehouse cursor\n",
    "    dw_cur = dw_conn.cursor()\n",
    "    print(f\"Connected to datawarehouse\")\n",
    "    return dw_conn, dw_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to restart the datawarehouse connection in case of errors\n",
    "def restart_connection(conn):\n",
    "    # close the current datawarehouse connection\n",
    "    conn.close()\n",
    "    # start new connection\n",
    "    dw_conn, dw_cur = connect_datawarehouse() \n",
    "    return dw_conn, dw_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tables for fact table and dimension table in the datawarehouse\n",
    "def create_tables(cur,conn,create_table_queries):\n",
    "    print(f\"Creating tables inside data warehouse\")\n",
    "    try:\n",
    "        for query in create_table_queries:\n",
    "            cur.execute(query)\n",
    "        conn.commit() \n",
    "        print(\"Tables created successfully\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Tables weren't dropped.\\n Following error encountered: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to drop all the existing tables in datawarehouse\n",
    "def drop_existing_tables(cur,conn,drop_table_queries):\n",
    "    print(\"Dropping existing tables from datawarehouse\")\n",
    "    try:\n",
    "        for query in drop_table_queries: \n",
    "            cur.execute(query)\n",
    "        conn.commit() \n",
    "        print(\"Existing tables dropped\")\n",
    "    except Exception as e:\n",
    "        conn.rollback() \n",
    "        print(f\"Tables weren't dropped.\\n Following error encountered: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to truncate all the existing tables in datawarehouse\n",
    "def truncate_existing_tables(cur,conn,truncate_table_queries):\n",
    "    print(\"Truncating existing tables from datawarehouse\")\n",
    "    try:\n",
    "        for query in truncate_table_queries: \n",
    "            cur.execute(query)\n",
    "        conn.commit() \n",
    "        print(\"Existing tables truncated\")\n",
    "    except Exception as e:\n",
    "        conn.rollback() \n",
    "        print(f\"Tables weren't truncated.\\n Following error encountered: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to insert data to the tables in datawarehouse\n",
    "def insert_data_to_tables(cur, conn, insert_table_queries,product_df,brand_df,reviewer_df,date_df,reviews_df):\n",
    "    last_val = 0\n",
    "    try:\n",
    "        # insert into product table from product df\n",
    "        for i, row in product_df.iterrows():\n",
    "            cur.execute(insert_table_queries[\"product\"],list(row))\n",
    "        print(\"Loaded data to product table\")\n",
    "        # insert into brands table from brand df\n",
    "        for i, row in brand_df.iterrows():\n",
    "            cur.execute(insert_table_queries[\"brand\"],list(row))\n",
    "        print(\"Loaded data to Brand table\")\n",
    "        # insert into reviewer table from reviewer df\n",
    "        for i, row in reviewer_df.iterrows():\n",
    "            last_val = i\n",
    "            cur.execute(insert_table_queries[\"reviewer\"],list(row))\n",
    "        print(\"Loaded data to Reviewer table\")\n",
    "        # insert into date table from date df \n",
    "        for i,row in date_df.iterrows():\n",
    "            cur.execute(insert_table_queries[\"date\"],list(row))\n",
    "        print(\"Loaded data to Date table\")\n",
    "        # insert into reviews table from reviews df \n",
    "        for i,row in reviews_df.iterrows():\n",
    "            last_val = i\n",
    "            cur.execute(insert_table_queries[\"reviews\"],list(row))\n",
    "        print(\"Loaded data to Product Reviews table\")\n",
    "        conn.commit()\n",
    "        print(\"All Data was inserted successfully into tables\")\n",
    "    except Exception as e:\n",
    "        print(last_val)\n",
    "        print(\"Query execution failed, so data wasn't inserted\")\n",
    "        print(f\"Following error occured: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "   print(\"Starting Extract Step . . .\")\n",
    "   df1 = pd.read_csv(dataset1_path)\n",
    "   df2 = pd.read_csv(dataset2_path,index_col=0)\n",
    "   # considering only the required columns from both dataframes\n",
    "   df1 = df1[['product_id', 'brand_id', 'loves_count','rating', 'reviews','primary_category','child_count']]\n",
    "   df2 = df2[['author_id', 'rating', 'submission_time', 'review_text',\n",
    "      'review_title', 'skin_tone', 'eye_color', 'skin_type', 'hair_color',\n",
    "      'product_id', 'product_name', 'brand_name', 'price_usd']]\n",
    "   print(\"Extract Step Completed!!!\")\n",
    "   return df1, df2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df1,df2):\n",
    "    print(\"Starting Transform Step . . .\")\n",
    "    # merge two datasets based on product_id \n",
    "    df_merged = pd.merge(df1,df2,how=\"inner\",on=\"product_id\")\n",
    "    # Rename Columns \n",
    "    rename_dict = {\n",
    "    \"loves_count\": \"favorites_count\",\n",
    "    \"rating_x\": \"avg_product_rating\",\n",
    "    \"reviews\": \"product_reviews_count\",\n",
    "    \"primary_category\": \"product_category\",\n",
    "    \"child_count\": \"variations_count\",\n",
    "    \"rating_y\": \"review_rating\",\n",
    "    \"submission_time\": \"full_date\",\n",
    "    \"skin_tone\": \"reviewer_skin_tone\",\n",
    "    \"skin_type\": \"reviewer_skin_type\",\n",
    "    \"eye_color\": \"reviewer_eye_color\",\n",
    "    \"hair_color\": \"reviewer_hair_color\",\n",
    "    \"price_usd\": \"product_price\",\n",
    "    \"author_id\": \"reviewer_id\"\n",
    "    }\n",
    "    df_merged = df_merged.rename(columns=rename_dict)\n",
    "    # cleaning nan values\n",
    "    # 1. drop rows with nan values in review_text as it is most essential for product_reviews\n",
    "    df_merged = df_merged.dropna(subset=[\"review_text\"])\n",
    "    #2. review_title is optional, so for rows with nan in review_title but certain values in review_text, fill the nan values with a default placeholder value\n",
    "    df_merged[\"review_title\"] = df_merged[\"review_title\"].fillna(\"Review Provided\")\n",
    "    # 3. For categorical columns like reviewer_skin_tone, reviewer_skin_type, reviewer_hair_color, reviewer_eye_color replace the nan values with mode\n",
    "    df_merged[\"reviewer_skin_tone\"] = df_merged[\"reviewer_skin_tone\"].fillna(df_merged[\"reviewer_skin_tone\"].mode()[0])\n",
    "    df_merged[\"reviewer_eye_color\"] = df_merged[\"reviewer_eye_color\"].fillna(df_merged[\"reviewer_eye_color\"].mode()[0])\n",
    "    df_merged[\"reviewer_skin_type\"] = df_merged[\"reviewer_skin_type\"].fillna(df_merged[\"reviewer_skin_type\"].mode()[0])\n",
    "    df_merged[\"reviewer_hair_color\"] = df_merged[\"reviewer_hair_color\"].fillna(df_merged[\"reviewer_hair_color\"].mode()[0])\n",
    "    # Creating new columns for date \n",
    "    df_merged[\"full_date\"] = pd.to_datetime(df_merged[\"full_date\"])\n",
    "    df_merged[\"year\"] = df_merged[\"full_date\"].dt.year\n",
    "    df_merged[\"month\"] = df_merged[\"full_date\"].dt.month\n",
    "    df_merged[\"day\"] = df_merged[\"full_date\"].dt.day\n",
    "    # Converting the values in reviewer_id column to int from object\n",
    "    # checking for non-numeric reviewer_id\n",
    "    df_merged[\"is_numeric\"] = df_merged[\"reviewer_id\"].apply(checkNumeric)\n",
    "    # removing non numeric reviewer ids\n",
    "    df_merged = df_merged[df_merged[\"is_numeric\"] == True]\n",
    "    # dropping the is_numeric column\n",
    "    df_merged.drop(columns=[\"is_numeric\"],inplace=True)\n",
    "    # converting reviewer_id to int\n",
    "    df_merged[\"reviewer_id\"] = df_merged[\"reviewer_id\"].apply(lambda x: int(x))\n",
    "    # Creating multiple dataframes for fact and dimension tables\n",
    "    # Product Reviews Table: Fact Table\n",
    "    reviews_df = df_merged[['product_id','brand_id','reviewer_id','full_date','review_title','review_text','review_rating']]\n",
    "    reviews_df = reviews_df.rename(columns={'full_date': 'date_id'})\n",
    "    reviews_df = reviews_df.reset_index(drop=True)\n",
    "    reviews_df.insert(0, 'review_id', reviews_df.index + 1)\n",
    "    # Product table: Dimension table\n",
    "    product_df = df_merged[['product_id', 'product_name', 'avg_product_rating', 'product_price', 'product_reviews_count', 'favorites_count', 'variations_count', 'product_category']]\n",
    "    # To keep only unique product descriptions in product_df\n",
    "    product_df = product_df.drop_duplicates(\"product_id\").reset_index(drop=True)\n",
    "    # Brand table: dimension table\n",
    "    brand_df = df_merged[['brand_id', 'brand_name']]\n",
    "    # To keep only the unique brand details in brands dataframe\n",
    "    brand_df = brand_df.drop_duplicates(\"brand_id\").reset_index(drop=True)\n",
    "    # Reviewer table: dimension table\n",
    "    reviewer_df = df_merged[['reviewer_id', 'reviewer_skin_tone', 'reviewer_skin_type', 'reviewer_eye_color', 'reviewer_hair_color']]\n",
    "    # To keep only unique reviewer details\n",
    "    reviewer_df = reviewer_df.drop_duplicates(\"reviewer_id\").reset_index(drop=True)\n",
    "    # Date table: dimension table\n",
    "    date_df = df_merged[['full_date', 'year', 'month', 'day']]\n",
    "    date_df = date_df.rename(columns={\"full_date\": \"date_id\"})\n",
    "    date_df = date_df.drop_duplicates(\"date_id\").reset_index(drop=True)\n",
    "    print(\"Transform Step Completed\")\n",
    "    # Return the dataframes for fact and dimension tables\n",
    "    return reviews_df, product_df, brand_df, reviewer_df, date_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(reviews_df,product_df,brand_df,reviewer_df,date_df):\n",
    "    print(\"Starting Load Step . . .\")\n",
    "    # firstly datawarehouse named etl_dw is created\n",
    "    create_datawarehouse()\n",
    "    try:\n",
    "        # Then we connect to this datawarehouse using the user credentials\n",
    "        dw_conn, dw_cur = connect_datawarehouse()\n",
    "    except:\n",
    "        dw_conn, dw_cur = restart_connection()\n",
    "    # Then create tables in etl_dw for fact and dimension tables\n",
    "    create_tables(dw_cur,dw_conn,CREATE_TABLE_QUERIES)\n",
    "    #----------------------------------------------------------------------\n",
    "    # # In some cases we may need to truncate the tables\n",
    "    # truncate_existing_tables(dw_cur,dw_conn,TRUNCATE_TABLE_QUERIES)\n",
    "    # # In some cases we may need to drop the tables\n",
    "    # drop_existing_tables(dw_cur,dw_conn,DROP_TABLE_QUERIES)\n",
    "    #----------------------------------------------------------------------\n",
    "    # To insert the data to database tables from dataframe\n",
    "    insert_data_to_tables(dw_cur, dw_conn, INSERT_TABLE_QUERIES,product_df,brand_df,reviewer_df,date_df,reviews_df)\n",
    "    # To close the connection with data warehouse\n",
    "    dw_conn.close()\n",
    "\n",
    "    print(\"Load Step Completed.\\nCheck your datawarehouse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl_pipeline():\n",
    "    df1,df2 = extract_data()\n",
    "    reviews_df, product_df, brand_df, reviewer_df, date_df = transform_data(df1,df2)\n",
    "    load_data(reviews_df, product_df, brand_df, reviewer_df, date_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started running etl pipeline\n",
      "Starting Extract Step . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18436\\1021977988.py:4: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(dataset2_path,index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Step Completed!!!\n",
      "Starting Transform Step . . .\n",
      "Transform Step Completed\n",
      "Starting Load Step . . .\n",
      "Started creating data warehouse\n",
      "Data warehouse created succesfully\n",
      "Connected to datawarehouse\n",
      "Creating tables inside data warehouse\n",
      "Tables created successfully\n",
      "Loaded data to product table\n",
      "Loaded data to Brand table\n",
      "Loaded data to Reviewer table\n",
      "Loaded data to Date table\n",
      "Loaded data to Product Reviews table\n",
      "All Data was inserted successfully into tables\n",
      "Load Step Completed.\n",
      "Check your datawarehouse\n",
      "ETL pipeline ran succesfully\n",
      "Data was succesfully loaded\n",
      " Total time taken: 33.44708037376404 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Started running etl pipeline\")\n",
    "run_etl_pipeline() \n",
    "end_time = time.time()\n",
    "print(f\"ETL pipeline ran succesfully\\nData was succesfully loaded\\n Total time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Outputs\n",
    "#### 1. Data Warehouse\n",
    "![Data Warehouse](diagrams/op1.png)\n",
    "#### 2. Tables in Data Warehouse\n",
    "![Data warehouse tables](diagrams/op2.png)\n",
    "#### 3. ER Diagram of Data Warehouse\n",
    "![ERD](diagrams/erd_datawarehouse.png)\n",
    "#### 4. Query results for Data Warehouse\n",
    "![Query1](diagrams/product_table_query_op.png)\n",
    "![Query2](diagrams/brand_table_query_op.png)\n",
    "![Query3](diagrams/reviewer_table_query_op.png)\n",
    "![Query4](diagrams/date_table_query_op.png)\n",
    "![Query5](diagrams/reviews_table_query_op.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".final_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
