{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL pipeline and Data warehouse using Python and Postgresql\n",
    "\n",
    "Here, I will create a simple ETL pipeline using Python and Postgresql and won't use AWS. \n",
    "\n",
    "### Basic ETL Workflow\n",
    "1. Extract data from CSV files \n",
    "2. Transform the data using Pandas\n",
    "3. Load the transformed data to a data warehouse created using Postgresql\n",
    "\n",
    "### For Data warehouse\n",
    "- Create Data Model illustrating star schema with fact and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Imports \n",
    "# sqlalchemy and psycopg2 will be used to connect to the postgresql database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "# pygrametl is used to create ETL flow\n",
    "import pygrametl \n",
    "# To read data from external csv files\n",
    "from pygrametl.datasources import CSVSource\n",
    "# Using pygrametl we can interact with dimensions and fat table using set of classes\n",
    "from pygrametl.tables import CachedDimension, FactTable \n",
    "# For data processing\n",
    "import pandas as pd \n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".final_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
