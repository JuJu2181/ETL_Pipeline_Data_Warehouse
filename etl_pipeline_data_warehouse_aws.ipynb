{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmenting an ETL pipeline and Data warehouse using AWS services like S3, Glue, Athena and Redshift\n",
    "- I will be using the same dataset I used for creating the etl pipeline and data warehouse using Python and Postgresql\n",
    "- In this notebook, I will be using boto3 which is AWS SDK for python to connect to aws account and access the AWS services.\n",
    "- Here, I am trying to practice Infrastructure as a Code(IaaC) in this project.\n",
    "\n",
    "## Main objective\n",
    "- To implement ETL pipeline and Data warehouse using AWS Services.\n",
    "- To learn and understand about AWS services"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To connect to AWS \n",
    "import boto3\n",
    "# To parse the data in config file\n",
    "import configparser\n",
    "# to process data\n",
    "import pandas as pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring AWS Account to be used from local jupyter notebook\n",
    "- I have created a configuration file dwh.cfg which includes all the necessary configurations for connecting to AWS account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the configparser object\n",
    "config = configparser.ConfigParser()\n",
    "# read the configuration from local file\n",
    "config.read_file(open('dwh.cfg'))\n",
    "# Load the parameters from the config file into variables\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "AWS_REGION             = config.get(\"AWS\",\"DEFAULT_REGION\")            \n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# * Checkout the parameters in a pandas dataframe\n",
    "# pd.DataFrame({\"Param\":\n",
    "#                   [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "#               \"Value\":\n",
    "#                   [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "#              })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 clients for S3, IAM, Glue and Redshift\n",
    "These clients will then be used to access those AWS services and perform various services with those services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boto3 clients created succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3 = boto3.client('s3',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "\n",
    "    iam = boto3.client('iam',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "\n",
    "    glue = boto3.client('glue',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "\n",
    "    redshift = boto3.client('redshift',\n",
    "                            region_name=AWS_REGION,\n",
    "                            aws_access_key_id=KEY,\n",
    "                            aws_secret_access_key=SECRET)\n",
    "    \n",
    "    print(\"Boto3 clients created succesfully\")\n",
    "except Exception as e:\n",
    "    print(\"Folowing error was encountered:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data from csv to parquet format before storing it in S3 bucket\n",
    "- Parquet format is chosen as it optimizes query performance by allowing for efficient column pruning and data skipping. Parquet stores data in columnar format and also uses compression algorithm that reduces storage costs and also improve query performance\n",
    "- To transform data from csv to parquet we use pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully converted to parquet format\n"
     ]
    }
   ],
   "source": [
    "# Read the csv dataset\n",
    "# Dataset Folder\n",
    "DATASET_FOLDER = 'datasets'\n",
    "\n",
    "df1 = pd.read_csv(f\"{DATASET_FOLDER}/product_info.csv\")\n",
    "# low_memory=False to deal with mixed datatypes warning\n",
    "df2 = pd.read_csv(f\"{DATASET_FOLDER}/product_reviews.csv\",index_col=0,low_memory=False)\n",
    "\n",
    "# Transform to parquet format\n",
    "df1_t = df1.to_parquet(f\"{DATASET_FOLDER}/product_info.parquet\")\n",
    "df2_t = df2.to_parquet(f\"{DATASET_FOLDER}/product_reviews.parquet\")\n",
    "print(\"Data succesfully converted to parquet format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using S3 service to upload the dataset to S3 bucket\n",
    "- List all existing buckets in aws account\n",
    "- Check if the required s3 bucket exists or not.\n",
    "- If not create a new bucket.\n",
    "- Upload the file from local directory to S3 bucket\n",
    "> Note: The S3 bucket name should be globally unique else you will get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coderush-anish-etl-source-data already exists!\n",
      "Started uploading the files . . .\n",
      "Successfully uploaded all files\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATA_BUCKET=\"coderush-anish-etl-source-data\"\n",
    "\n",
    "try:\n",
    "    # get list of all available buckets\n",
    "    response = s3.list_buckets()\n",
    "    buckets_list = []\n",
    "    for res in response['Buckets']:\n",
    "        buckets_list.append(res['Name'])\n",
    "    # print(buckets_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching the s3 buckets. Following error encountered:\\n{e}\")\n",
    "    \n",
    "# Create S3 bucket if not exists\n",
    "if SOURCE_DATA_BUCKET not in buckets_list:\n",
    "    print(f\"Creating {SOURCE_DATA_BUCKET} . . .\")\n",
    "    try:\n",
    "        response = s3.create_bucket(\n",
    "            Bucket=SOURCE_DATA_BUCKET,\n",
    "            CreateBucketConfiguration={\n",
    "                    'LocationConstraint': 'ap-south-1'\n",
    "                    }\n",
    "        )\n",
    "        print(f\"{SOURCE_DATA_BUCKET} created successfully !\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create {SOURCE_DATA_BUCKET}. Following error encountered:\\n{e}\")\n",
    "else:\n",
    "    print(f\"{SOURCE_DATA_BUCKET} already exists!\")\n",
    "\n",
    "# Uploading the dataset files from local directory to s3 bucket\n",
    "try:\n",
    "    print(\"Started uploading the files . . .\")\n",
    "    files_to_upload = [\"product_info.parquet\",\"product_reviews.parquet\"]\n",
    "    for file_name in files_to_upload:\n",
    "        s3.upload_file(f\"{DATASET_FOLDER}/{file_name}\",SOURCE_DATA_BUCKET,file_name)\n",
    "    print('Successfully uploaded all files')\n",
    "except Exception as e: \n",
    "    print(f\"Failed to upload datasets to s3 bucket.\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".final_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
