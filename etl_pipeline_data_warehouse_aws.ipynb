{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmenting an ETL pipeline and Data warehouse using AWS services like S3, Glue, Athena and Redshift\n",
    "- I will be using the same dataset I used for creating the etl pipeline and data warehouse using Python and Postgresql\n",
    "- In this notebook, I will be using boto3 which is AWS SDK for python to connect to aws account and access the AWS services.\n",
    "- Here, I am trying to practice Infrastructure as a Code(IaaC) in this project.\n",
    "\n",
    "## Main objective\n",
    "- To implement ETL pipeline and Data warehouse using AWS Services.\n",
    "- To learn and understand about AWS services"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To connect to AWS \n",
    "import boto3\n",
    "# To parse the data in config file\n",
    "import configparser\n",
    "# to process data\n",
    "import pandas as pd \n",
    "# using json\n",
    "import json\n",
    "# to connect to Redshift cluster\n",
    "import psycopg2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring AWS Account to be used from local jupyter notebook\n",
    "- I have created a configuration file dwh.cfg which includes all the necessary configurations for connecting to AWS account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the configparser object\n",
    "config = configparser.ConfigParser()\n",
    "# read the configuration from local file\n",
    "config.read_file(open('dwh.cfg'))\n",
    "# Load the parameters from the config file into variables\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "AWS_REGION             = config.get(\"AWS\",\"DEFAULT_REGION\")            \n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# * Checkout the parameters in a pandas dataframe\n",
    "# pd.DataFrame({\"Param\":\n",
    "#                   [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "#               \"Value\":\n",
    "#                   [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "#              })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 clients for S3, IAM, Glue and Redshift\n",
    "These clients will then be used to access those AWS services and perform various services with those services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boto3 clients created succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # ec2 is needed for creating and configuring security group for redshift\n",
    "    ec2 = boto3.client('ec2',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "\n",
    "    iam = boto3.client('iam',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "\n",
    "    glue = boto3.client('glue',\n",
    "                        region_name=AWS_REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)\n",
    "    \n",
    "    athena = boto3.client('athena',\n",
    "                    region_name=AWS_REGION,\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET)\n",
    "\n",
    "    redshift = boto3.client('redshift',\n",
    "                            region_name=AWS_REGION,\n",
    "                            aws_access_key_id=KEY,\n",
    "                            aws_secret_access_key=SECRET)\n",
    "    \n",
    "    print(\"Boto3 clients created succesfully\")\n",
    "except Exception as e:\n",
    "    print(\"Folowing error was encountered:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data from csv to parquet format before storing it in S3 bucket\n",
    "- Parquet format is chosen as it optimizes query performance by allowing for efficient column pruning and data skipping. Parquet stores data in columnar format and also uses compression algorithm that reduces storage costs and also improve query performance\n",
    "- To transform data from csv to parquet we use pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully converted to parquet format\n"
     ]
    }
   ],
   "source": [
    "# Read the csv dataset\n",
    "# Dataset Folder\n",
    "DATASET_FOLDER = 'datasets'\n",
    "\n",
    "df1 = pd.read_csv(f\"{DATASET_FOLDER}/product_info.csv\")\n",
    "# low_memory=False to deal with mixed datatypes warning\n",
    "df2 = pd.read_csv(f\"{DATASET_FOLDER}/product_reviews.csv\",index_col=0,low_memory=False)\n",
    "\n",
    "# Transform to parquet format\n",
    "df1_t = df1.to_parquet(f\"{DATASET_FOLDER}/product_info.parquet\")\n",
    "df2_t = df2.to_parquet(f\"{DATASET_FOLDER}/product_reviews.parquet\")\n",
    "print(\"Data succesfully converted to parquet format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using S3 service to upload the dataset to S3 bucket\n",
    "- List all existing buckets in aws account\n",
    "- Check if the required s3 bucket exists or not.\n",
    "- If not create a new bucket.\n",
    "- Upload the file from local directory to S3 bucket\n",
    "> Note: The S3 bucket name should be globally unique else you will get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_BUCKET=\"coderush-anish-etl-source-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload files to s3 bucket\n",
    "def upload_files_to_s3_bucket(files_to_upload: list):\n",
    "    # Uploading the dataset files from local directory to s3 bucket\n",
    "    try:\n",
    "        print(\"Started uploading the files . . .\")\n",
    "        # print(files_to_upload)\n",
    "        for file_name in files_to_upload:\n",
    "            s3.upload_file(f\"{DATASET_FOLDER}/{file_name}\",SOURCE_DATA_BUCKET,file_name)\n",
    "        print('Successfully uploaded all files')\n",
    "    except Exception as e: \n",
    "        print(f\"Failed to upload datasets to s3 bucket.\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coderush-anish-etl-source-data already exists!\n",
      "Started uploading the files . . .\n",
      "Successfully uploaded all files\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATA_BUCKET=\"coderush-anish-etl-source-data\"\n",
    "\n",
    "try:\n",
    "    # get list of all available buckets\n",
    "    response = s3.list_buckets()\n",
    "    # print(response) \n",
    "    #? Response looks like this\n",
    "    # {'ResponseMetadata': {'RequestId': 'YMYR3MA9HN636GKE', 'HostId': 'GdfIlZp7Pd2k1SGdSHeLzPac01j/HoJRtilc/m3h5HMqjfBvZ9wqbIU3fIAObcJ6Xjc2HNVMJ4s=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'GdfIlZp7Pd2k1SGdSHeLzPac01j/HoJRtilc/m3h5HMqjfBvZ9wqbIU3fIAObcJ6Xjc2HNVMJ4s=', 'x-amz-request-id': 'YMYR3MA9HN636GKE', 'date': 'Sun, 14 May 2023 08:09:14 GMT', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'Buckets': [{'Name': 'anish-shilpakar-ccp-2022-demo', 'CreationDate': datetime.datetime(2023, 1, 16, 9, 40, 15, tzinfo=tzutc())}, {'Name': 'anish-shilpakar-replication-demo', 'CreationDate': datetime.datetime(2023, 1, 16, 9, 28, 12, tzinfo=tzutc())}, {'Name': 'anish-shilpakar-server-access-logs', 'CreationDate': datetime.datetime(2023, 1, 16, 9, 12, 36, tzinfo=tzutc())}, {'Name': 'coderush-anish-etl-source-data', 'CreationDate': datetime.datetime(2023, 5, 14, 7, 39, 17, tzinfo=tzutc())}, {'Name': 'elasticbeanstalk-ap-south-1-977481651193', 'CreationDate': datetime.datetime(2023, 1, 17, 12, 31, 59, tzinfo=tzutc())}], 'Owner': {'ID': 'ba72a4b013b258edfa3487309159137f7a6ebb75e63e86982391186c8ca1bf29'}}\n",
    "    buckets_list = []\n",
    "    for res in response['Buckets']:\n",
    "        buckets_list.append(res['Name'])\n",
    "    # print(buckets_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching the s3 buckets. Following error encountered:\\n{e}\")\n",
    "    \n",
    "# Create S3 bucket if not exists\n",
    "if SOURCE_DATA_BUCKET not in buckets_list:\n",
    "    print(f\"Creating {SOURCE_DATA_BUCKET} . . .\")\n",
    "    try:\n",
    "        response = s3.create_bucket(\n",
    "            Bucket=SOURCE_DATA_BUCKET,\n",
    "            CreateBucketConfiguration={\n",
    "                    'LocationConstraint': 'ap-south-1'\n",
    "                    }\n",
    "        )\n",
    "        print(f\"{SOURCE_DATA_BUCKET} created successfully !\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create {SOURCE_DATA_BUCKET}. Following error encountered:\\n{e}\")\n",
    "else:\n",
    "    print(f\"{SOURCE_DATA_BUCKET} already exists!\")\n",
    "\n",
    "# Uploading the dataset files from local directory to s3 bucket\n",
    "upload_files_to_s3_bucket([\"product_info.parquet\",\"product_reviews.parquet\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Glue using Boto3 to perform ETL i-e Extract Step\n",
    "1. Create glue database\n",
    "2. Create Glue Crawler to automatically extract the schema of the dataset files to create Glue Datacatalog.\n",
    "3. This should create tables for the data files in Glue in which the data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_project_db already exists\n"
     ]
    }
   ],
   "source": [
    "DATABASE_NAME = \"final_project_db\"\n",
    "# check if the database already exists or not\n",
    "try:\n",
    "    response = glue.get_databases()\n",
    "    # print(response)\n",
    "    # ? Response looks like this\n",
    "    # {'DatabaseList': [{'Name': 'testdb', 'CreateTime': datetime.datetime(2023, 5, 14, 13, 52, 22, tzinfo=tzlocal()), 'CreateTableDefaultPermissions': [{'Principal': {'DataLakePrincipalIdentifier': 'IAM_ALLOWED_PRINCIPALS'}, 'Permissions': ['ALL']}], 'CatalogId': '977481651193'}], 'ResponseMetadata': {'RequestId': '0548b52d-3533-4c64-8a16-577a6b7e94aa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 14 May 2023 08:07:26 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '249', 'connection': 'keep-alive', 'x-amzn-requestid': '0548b52d-3533-4c64-8a16-577a6b7e94aa'}, 'RetryAttempts': 0}}\n",
    "    databases_list = []\n",
    "    for db in response['DatabaseList']:\n",
    "        databases_list.append(db['Name'])\n",
    "    # print(databases_list)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get the databases list.Following error occured:\\n{e}\")\n",
    "\n",
    "if DATABASE_NAME not in databases_list:\n",
    "# create database using boto3\n",
    "    try:\n",
    "        print(f\"Started creating {DATABASE_NAME} . . .\")\n",
    "        response = glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': DATABASE_NAME,\n",
    "            }\n",
    "        )\n",
    "        print(f\"{DATABASE_NAME} created successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to created Glue database. Following error occured:\\n{e}\")\n",
    "else: \n",
    "    print(f\"{DATABASE_NAME} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created crawler final_data_crawler\n"
     ]
    }
   ],
   "source": [
    "CRAWLER_NAME = \"final_data_crawler\"\n",
    "ROLE_NAME=\"glue-crawler-role\"\n",
    "S3_PATH=\"s3://coderush-anish-etl-source-data/product_reviews.parquet\"\n",
    "# check if the crawler already exists in glue\n",
    "try:\n",
    "    response = glue.get_crawlers()\n",
    "    # print(response)\n",
    "    # ? Response looks like this\n",
    "    # {'Crawlers': [{'Name': 'test-crawler', 'Role': 'glue-crawler-role', 'Targets': {'S3Targets': [{'Path': 's3://coderush-anish-etl-source-data/product_info.parquet', 'Exclusions': []}], 'JdbcTargets': [], 'MongoDBTargets': [], 'DynamoDBTargets': [], 'CatalogTargets': [], 'DeltaTargets': []}, 'DatabaseName': 'final_project_db', 'Classifiers': [], 'RecrawlPolicy': {'RecrawlBehavior': 'CRAWL_EVERYTHING'}, 'SchemaChangePolicy': {'UpdateBehavior': 'UPDATE_IN_DATABASE', 'DeleteBehavior': 'DEPRECATE_IN_DATABASE'}, 'LineageConfiguration': {'CrawlerLineageSettings': 'DISABLE'}, 'State': 'RUNNING', 'CrawlElapsedTime': 5770, 'CreationTime': datetime.datetime(2023, 5, 14, 18, 30, 35, tzinfo=tzlocal()), 'LastUpdated': datetime.datetime(2023, 5, 14, 18, 30, 35, tzinfo=tzlocal()), 'Version': 1, 'Configuration': '{\"Version\":1.0,\"CreatePartitionIndex\":true}', 'LakeFormationConfiguration': {'UseLakeFormationCredentials': False, 'AccountId': ''}}], 'ResponseMetadata': {'RequestId': 'd0c90f61-4f17-49d5-b106-4be2e923a11a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 14 May 2023 12:45:44 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '861', 'connection': 'keep-alive', 'x-amzn-requestid': 'd0c90f61-4f17-49d5-b106-4be2e923a11a'}, 'RetryAttempts': 0}}\n",
    "    crawlers_list = []\n",
    "    for crawler in response['Crawlers']:\n",
    "        crawlers_list.append(crawler['Name'])\n",
    "    # print(crawlers_list)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get the crawlers list.Following error occured:\\n{e}\")\n",
    "\n",
    "# check if the crawler already exists before creating the cluster\n",
    "if CRAWLER_NAME not in crawlers_list:\n",
    "    try:\n",
    "        response = glue.create_crawler(\n",
    "            Name=CRAWLER_NAME,\n",
    "            Role=ROLE_NAME,\n",
    "            DatabaseName=DATABASE_NAME,\n",
    "            Description=\"AWS Glue crawler to crawl the source data\",\n",
    "            Targets={\n",
    "                'S3Targets': [\n",
    "                    {\n",
    "                        'Path': S3_PATH,\n",
    "                        'Exclusions': [\n",
    "                            'string',\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"Successfully created crawler {CRAWLER_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create the crawler {CRAWLER_NAME}\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'f5387ba3-ffd5-44c8-9f3a-80ef78cf0570', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 14 May 2023 12:56:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'f5387ba3-ffd5-44c8-9f3a-80ef78cf0570'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Start running crawler\n",
    "try:\n",
    "    response = glue.start_crawler(\n",
    "        Name=CRAWLER_NAME\n",
    "    )\n",
    "    # print(response)\n",
    "    print(f\"Successfully started running Glue crawler {CRAWLER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error when starting crawler.\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR: An issue I am facing, don't know if it is an issue of glue or issue of dataset, but I can create glue crawler and then create tables using that crawler but after that when I query the table using Athena only the column names are visible and there is no data loaded inside that table.   \n",
    "\n",
    "So for now I will be doing the transform step using pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Glue tables using Amazon Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QueryExecutionId': '92456b91-cfc3-49f4-82ce-abf2d315a7cd', 'ResponseMetadata': {'RequestId': 'ebd42f7e-6524-437c-9829-f4855efd2910', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 14 May 2023 13:09:52 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '59', 'connection': 'keep-alive', 'x-amzn-requestid': 'ebd42f7e-6524-437c-9829-f4855efd2910'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "S3_STAGING_DIR = \"s3://anish-shilpakar-athena-query-results/output/\"\n",
    "response = athena.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM product_info_parquet\",\n",
    "    QueryExecutionContext = {\"Database\":DATABASE_NAME},\n",
    "    ResultConfiguration = {\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "S3_BUCKET_NAME=\"anish-shilpakar-athena-query-results\"\n",
    "S3_OUTPUT_DIRECTORY=\"output\"\n",
    "Dict = {}\n",
    "def download_and_load_query_results(client: boto3.client, query_response: Dict) -> pd.DataFrame:\n",
    "    while True:\n",
    "        try:\n",
    "            #This function only loads the first 1000 rows\n",
    "            client.get_query_results(\n",
    "                QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as err:\n",
    "            if \"not yet finished\" in str(err):\n",
    "                time.sleep(0.001)\n",
    "            else:\n",
    "                raise err\n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3.download_file(\n",
    "        S3_BUCKET_NAME,\n",
    "        f\"{S3_OUTPUT_DIRECTORY}/{query_response['QueryExecutionId']}.csv\",\n",
    "        temp_file_location,\n",
    "    )\n",
    "    return pd.read_csv(temp_file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>loves_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>size</th>\n",
       "      <th>variation_type</th>\n",
       "      <th>variation_value</th>\n",
       "      <th>...</th>\n",
       "      <th>online_only</th>\n",
       "      <th>out_of_stock</th>\n",
       "      <th>sephora_exclusive</th>\n",
       "      <th>highlights</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>secondary_category</th>\n",
       "      <th>tertiary_category</th>\n",
       "      <th>child_count</th>\n",
       "      <th>child_max_price</th>\n",
       "      <th>child_min_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product_id, product_name, brand_id, brand_name, loves_count, rating, reviews, size, variation_type, variation_value, variation_desc, ingredients, price_usd, value_price_usd, sale_price_usd, limited_edition, new, online_only, out_of_stock, sephora_exclusive, highlights, primary_category, secondary_category, tertiary_category, child_count, child_max_price, child_min_price]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_results = download_and_load_query_results(athena,response)\n",
    "athena_results.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see for some reason data is not being loaded in the glue tables even after running the glue crawler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Transform Steps\n",
    "### So instead using Pandas to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_path = \"datasets/product_info.csv\"\n",
    "dataset2_path = \"datasets/product_reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will check whether the value is a numeric value or not\n",
    "def checkNumeric(val):\n",
    "    return True if str(val).isdigit() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    print(\"Starting Extract Step . . .\")\n",
    "    df1 = pd.read_csv(dataset1_path)\n",
    "    df2 = pd.read_csv(dataset2_path,index_col=0)\n",
    "    # considering only the required columns from both dataframes\n",
    "    df1 = df1[['product_id', 'brand_id', 'loves_count','rating', 'reviews','primary_category','child_count']]\n",
    "    df2 = df2[['author_id', 'rating', 'submission_time', 'review_text',\n",
    "        'review_title', 'skin_tone', 'eye_color', 'skin_type', 'hair_color',\n",
    "        'product_id', 'product_name', 'brand_name', 'price_usd']]\n",
    "    print(\"Extract Step Completed!!!\")\n",
    "    return df1, df2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df1,df2):\n",
    "    print(\"Starting Transform Step . . .\")\n",
    "    # merge two datasets based on product_id \n",
    "    df_merged = pd.merge(df1,df2,how=\"inner\",on=\"product_id\")\n",
    "    # Rename Columns \n",
    "    rename_dict = {\n",
    "    \"loves_count\": \"favorites_count\",\n",
    "    \"rating_x\": \"avg_product_rating\",\n",
    "    \"reviews\": \"product_reviews_count\",\n",
    "    \"primary_category\": \"product_category\",\n",
    "    \"child_count\": \"variations_count\",\n",
    "    \"rating_y\": \"review_rating\",\n",
    "    \"submission_time\": \"full_date\",\n",
    "    \"skin_tone\": \"reviewer_skin_tone\",\n",
    "    \"skin_type\": \"reviewer_skin_type\",\n",
    "    \"eye_color\": \"reviewer_eye_color\",\n",
    "    \"hair_color\": \"reviewer_hair_color\",\n",
    "    \"price_usd\": \"product_price\",\n",
    "    \"author_id\": \"reviewer_id\"\n",
    "    }\n",
    "    df_merged = df_merged.rename(columns=rename_dict)\n",
    "    # cleaning nan values\n",
    "    # 1. drop rows with nan values in review_text as it is most essential for product_reviews\n",
    "    df_merged = df_merged.dropna(subset=[\"review_text\"])\n",
    "    #2. review_title is optional, so for rows with nan in review_title but certain values in review_text, fill the nan values with a default placeholder value\n",
    "    df_merged[\"review_title\"] = df_merged[\"review_title\"].fillna(\"Review Provided\")\n",
    "    # 3. For categorical columns like reviewer_skin_tone, reviewer_skin_type, reviewer_hair_color, reviewer_eye_color replace the nan values with mode\n",
    "    df_merged[\"reviewer_skin_tone\"] = df_merged[\"reviewer_skin_tone\"].fillna(df_merged[\"reviewer_skin_tone\"].mode()[0])\n",
    "    df_merged[\"reviewer_eye_color\"] = df_merged[\"reviewer_eye_color\"].fillna(df_merged[\"reviewer_eye_color\"].mode()[0])\n",
    "    df_merged[\"reviewer_skin_type\"] = df_merged[\"reviewer_skin_type\"].fillna(df_merged[\"reviewer_skin_type\"].mode()[0])\n",
    "    df_merged[\"reviewer_hair_color\"] = df_merged[\"reviewer_hair_color\"].fillna(df_merged[\"reviewer_hair_color\"].mode()[0])\n",
    "    # Creating new columns for date \n",
    "    df_merged[\"full_date\"] = pd.to_datetime(df_merged[\"full_date\"])\n",
    "    df_merged[\"year\"] = df_merged[\"full_date\"].dt.year\n",
    "    df_merged[\"month\"] = df_merged[\"full_date\"].dt.month\n",
    "    df_merged[\"day\"] = df_merged[\"full_date\"].dt.day\n",
    "    # Converting the values in reviewer_id column to int from object\n",
    "    # checking for non-numeric reviewer_id\n",
    "    df_merged[\"is_numeric\"] = df_merged[\"reviewer_id\"].apply(checkNumeric)\n",
    "    # removing non numeric reviewer ids\n",
    "    df_merged = df_merged[df_merged[\"is_numeric\"] == True]\n",
    "    # dropping the is_numeric column\n",
    "    df_merged.drop(columns=[\"is_numeric\"],inplace=True)\n",
    "    # converting reviewer_id to int\n",
    "    df_merged[\"reviewer_id\"] = df_merged[\"reviewer_id\"].apply(lambda x: int(x))\n",
    "    # Creating multiple dataframes for fact and dimension tables\n",
    "    # Product Reviews Table: Fact Table\n",
    "    reviews_df = df_merged[['product_id','brand_id','reviewer_id','full_date','review_title','review_text','review_rating']]\n",
    "    reviews_df = reviews_df.rename(columns={'full_date': 'date_id'})\n",
    "    reviews_df = reviews_df.reset_index(drop=True)\n",
    "    reviews_df.insert(0, 'review_id', reviews_df.index + 1)\n",
    "    # Product table: Dimension table\n",
    "    product_df = df_merged[['product_id', 'product_name', 'avg_product_rating', 'product_price', 'product_reviews_count', 'favorites_count', 'variations_count', 'product_category']]\n",
    "    # To keep only unique product descriptions in product_df\n",
    "    product_df = product_df.drop_duplicates(\"product_id\").reset_index(drop=True)\n",
    "    # Brand table: dimension table\n",
    "    brand_df = df_merged[['brand_id', 'brand_name']]\n",
    "    # To keep only the unique brand details in brands dataframe\n",
    "    brand_df = brand_df.drop_duplicates(\"brand_id\").reset_index(drop=True)\n",
    "    # Reviewer table: dimension table\n",
    "    reviewer_df = df_merged[['reviewer_id', 'reviewer_skin_tone', 'reviewer_skin_type', 'reviewer_eye_color', 'reviewer_hair_color']]\n",
    "    # To keep only unique reviewer details\n",
    "    reviewer_df = reviewer_df.drop_duplicates(\"reviewer_id\").reset_index(drop=True)\n",
    "    # Date table: dimension table\n",
    "    date_df = df_merged[['full_date', 'year', 'month', 'day']]\n",
    "    date_df = date_df.rename(columns={\"full_date\": \"date_id\"})\n",
    "    date_df = date_df.drop_duplicates(\"date_id\").reset_index(drop=True)\n",
    "    print(\"Transform Step Completed\")\n",
    "    # Return the dataframes for fact and dimension tables\n",
    "    result = {\n",
    "        \"reviews\": reviews_df,\n",
    "        \"product\": product_df,\n",
    "        \"brand\": brand_df,\n",
    "        \"reviewer\": reviewer_df,\n",
    "        \"date\": date_df\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_csvs(*args):\n",
    "    for k,v in args[0].items():\n",
    "        print(f\"Saving {k}.csv . . .\")\n",
    "        v.to_csv(f\"{DATASET_FOLDER}/{k}.csv\",index=False)\n",
    "    print(\"Files saved succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Extract Step . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7804\\3297522506.py:4: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(dataset2_path,index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Step Completed!!!\n",
      "Starting Transform Step . . .\n",
      "Transform Step Completed\n",
      "Saving reviews.csv . . .\n",
      "Saving product.csv . . .\n",
      "Saving brand.csv . . .\n",
      "Saving reviewer.csv . . .\n",
      "Saving date.csv . . .\n",
      "Files saved succesfully\n"
     ]
    }
   ],
   "source": [
    "# ET using pandas\n",
    "df1, df2 = extract_data()\n",
    "result = transform_data(df1,df2)\n",
    "save_df_to_csvs(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally loading the transformed csv files to redshift datawarehouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly uploading the transformed data files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started uploading the files . . .\n",
      "Successfully uploaded all files\n"
     ]
    }
   ],
   "source": [
    "files_to_upload = [f\"{name}.csv\" for name in result.keys()]\n",
    "upload_files_to_s3_bucket(files_to_upload)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then Creating IAM role that makes Redshift able to access S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "Role dwhRole created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create the IAM role\n",
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "    RoleName = DWH_IAM_ROLE_NAME,\n",
    "    Description = 'Allows Redshift cluster to call AWS service on your behalf.',\n",
    "    AssumeRolePolicyDocument = json.dumps(\n",
    "        {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                       'Effect': 'Allow', \n",
    "                       'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "         'Version': '2012-10-17'})\n",
    "    )\n",
    "    print(f'Role {DWH_IAM_ROLE_NAME} created successfully')\n",
    "except Exception as e:\n",
    "    print(f\"Failure in creating new role {DWH_IAM_ROLE_NAME}.\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach Policy to the role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 Attaching Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach Policy\n",
    "print('1.2 Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                          PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                          )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ARN for IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::977481651193:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# Get and print the IAM role ARN\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started creating Redshift cluster . . .\n",
      "Successfully created Redshift cluster: product_reviews_dw\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Started creating Redshift cluster . . .\")\n",
    "    response = redshift.create_cluster(        \n",
    "        # Hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        # Identifiers & credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        # Role (to allow s3 access)\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "    print(f\"Successfully created Redshift cluster: {DWH_DB}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failure when creating Redshift cluster {DWH_DB}\\nFollowing error occured:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Redshift cluster\n",
    "To continue loading data to the Redshift datawarehouse, the status of cluster should be changed to available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>anish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>product_reviews_dw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.cyoryyfhsxbk.ap-south-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-0a106004324381a9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key                                              Value\n",
       "0  ClusterIdentifier                                         dwhcluster\n",
       "1           NodeType                                          dc2.large\n",
       "2      ClusterStatus                                          available\n",
       "3     MasterUsername                                              anish\n",
       "4             DBName                                 product_reviews_dw\n",
       "5           Endpoint  {'Address': 'dwhcluster.cyoryyfhsxbk.ap-south-...\n",
       "6              VpcId                              vpc-0a106004324381a9d\n",
       "7      NumberOfNodes                                                  4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkRedshiftClusterStatus(props):\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "checkRedshiftClusterStatus(myClusterProps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Note of cluster endpint and role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cyoryyfhsxbk.ap-south-1.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::977481651193:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open an incoming TCP port to access the cluster endpoint \n",
    "For this we create a new security group using ec2 client and add inbound rules with TCP port open for all ip addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ec2 resource using boto3 to configure inbound rules for the security group\n",
    "ec2_r = ec2 = boto3.resource('ec2', \n",
    "                   region_name=AWS_REGION,\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vpc = ec2_r.Vpc(id=myClusterProps['VpcId'])\n",
    "# defaultSg = list(vpc.security_groups.all())[0]\n",
    "# defaultSg.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security group created and configured\n",
      "Inbound port configured successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    response = ec2.create_security_group(\n",
    "    Description='Security group for redshift cluster',\n",
    "    GroupName='redshift-sg'\n",
    "    )\n",
    "    # print(response)\n",
    "    # ec2.SecurityGroup(id='sg-0435581179b364351')\n",
    "    print(\"Security group created and configured\")\n",
    "    # Adding inbound rule to open an incoming TCP port to access the cluster endpoint\n",
    "    response.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,  \n",
    "        CidrIp='0.0.0.0/0',  \n",
    "        IpProtocol='TCP', \n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "    print(\"Inbound port configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failure when creating security group. Following error occured:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the security group of Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster': {'ClusterIdentifier': 'dwhcluster', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Modifying', 'MasterUsername': 'anish', 'DBName': 'product_reviews_dw', 'Endpoint': {'Address': 'dwhcluster.cyoryyfhsxbk.ap-south-1.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2023, 5, 14, 13, 53, 26, 872000, tzinfo=tzutc()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-03c149c7605c92e12', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-0a106004324381a9d', 'AvailabilityZone': 'ap-south-1c', 'PreferredMaintenanceWindow': 'wed:06:00-wed:06:30', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 4, 'PubliclyAccessible': True, 'Encrypted': False, 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::977481651193:role/dwhRole', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 5, 17, 6, 0, tzinfo=tzutc()), 'TotalStorageCapacityInMegaBytes': 1600000, 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}, 'ResponseMetadata': {'RequestId': 'df5ef751-49b0-4fb2-8c58-237df7e301dc', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'df5ef751-49b0-4fb2-8c58-237df7e301dc', 'content-type': 'text/xml', 'content-length': '2728', 'date': 'Sun, 14 May 2023 16:22:03 GMT'}, 'RetryAttempts': 0}}\n",
      "Successfully modified the security group of redshift cluster\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = redshift.modify_cluster(\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        VpcSecurityGroupIds=[\n",
    "        defaultSg.id\n",
    "        ]\n",
    "    ) \n",
    "    print(response)\n",
    "    print(\"Successfully modified the security group of redshift cluster\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to modify the security group of redshift cluster. Following error occured:\\n{e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Redshift cluster using psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected succesfully to Redshift cluster\n"
     ]
    }
   ],
   "source": [
    "# Set the connection parameters\n",
    "host = DWH_ENDPOINT\n",
    "port = DWH_PORT\n",
    "dbname = DWH_DB\n",
    "user = DWH_DB_USER\n",
    "password = DWH_DB_PASSWORD\n",
    "\n",
    "# Connect to the Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "print(\"Connected succesfully to Redshift cluster\")\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tables for fact and dimension tables in Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table 0\n",
      "Creating table 1\n",
      "Creating table 2\n",
      "Creating table 3\n",
      "Creating table 4\n",
      "All tables created successfully\n"
     ]
    }
   ],
   "source": [
    "CREATE_TABLE_QUERIES = [\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_product(\n",
    "        product_id TEXT PRIMARY KEY,\n",
    "        product_name TEXT,\n",
    "        avg_product_rating FLOAT,\n",
    "        product_price FLOAT,\n",
    "        product_reviews_count FLOAT,\n",
    "        favorites_count INT,\n",
    "        variations_count INT,\n",
    "        product_category TEXT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tbl_brand(\n",
    "        brand_id INT PRIMARY KEY,\n",
    "        brand_name TEXT UNIQUE\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_reviewer(\n",
    "        reviewer_id BIGINT PRIMARY KEY,\n",
    "        reviewer_skin_tone TEXT,\n",
    "        reviewer_skin_type TEXT,\n",
    "        reviewer_eye_color TEXT,\n",
    "        reviewer_hair_color TEXT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_date(\n",
    "        date_id TIMESTAMP PRIMARY KEY,\n",
    "        year INT,\n",
    "        month INT,\n",
    "        day INT\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    CREATE TABLE IF NOT EXISTS tbl_product_reviews(\n",
    "        review_id INT PRIMARY KEY,\n",
    "        product_id TEXT REFERENCES tbl_product(product_id),\n",
    "        brand_id INT REFERENCES tbl_brand(brand_id),\n",
    "        reviewer_id BIGINT REFERENCES tbl_reviewer(reviewer_id),\n",
    "        date_id TIMESTAMP REFERENCES tbl_date(date_id),\n",
    "        review_title TEXT,\n",
    "        review_text VARCHAR(512),\n",
    "        review_rating INT\n",
    "    )\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "for i,query in enumerate(CREATE_TABLE_QUERIES):\n",
    "    print(f\"Creating table {i}\")\n",
    "    # Create a table\n",
    "    cur.execute(query)\n",
    "\n",
    "print(\"All tables created successfully\")\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from S3 bucket to the tables in cluster\n",
    "Here, we use COPY command to load the data from S3 bucket to the tables in redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted to brand table\n"
     ]
    }
   ],
   "source": [
    "copy_query1 = \"\"\" \n",
    "COPY tbl_brand FROM 's3://coderush-anish-etl-source-data/brand.csv'\n",
    "CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' DELIMITER ',' REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(copy_query1)\n",
    "conn.commit()\n",
    "print(\"Data inserted to brand table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data to table 0\n",
      "Inserting data to table 1\n",
      "Inserting data to table 2\n",
      "Inserting data to table 3\n",
      "Inserting data to table 4\n",
      "Data inserted to all tables successfully\n"
     ]
    }
   ],
   "source": [
    "# queries to load the data from csv files stored in s3 bucket to the tables in datawarehouse\n",
    "LOAD_DATA_QUERIES = [\n",
    "    \"\"\" \n",
    "    COPY tbl_brand FROM 's3://coderush-anish-etl-source-data/brand.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' FORMAT CSV REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    COPY tbl_date FROM 's3://coderush-anish-etl-source-data/date.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' FORMAT CSV REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    COPY tbl_product FROM 's3://coderush-anish-etl-source-data/product.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' FORMAT CSV REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    COPY tbl_reviewer FROM 's3://coderush-anish-etl-source-data/reviewer.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' FORMAT CSV REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    COPY tbl_product_reviews FROM 's3://coderush-anish-etl-source-data/reviews.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::977481651193:role/dwhRole' FORMAT CSV REGION 'ap-south-1' IGNOREHEADER 1;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for i,query in enumerate(LOAD_DATA_QUERIES):\n",
    "        print(f\"Inserting data to table {i}\")\n",
    "        # Create a table\n",
    "        cur.execute(query)\n",
    "\n",
    "    print(\"Data inserted to all tables successfully\")\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to insert data to tables. Following error was encountered:\\n{e}\")\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally after loading the data into Redshift data warehouse data was queried to check if it has been properly loaded or not. The following output were obtained\n",
    "\n",
    "### Outputs in Redshift Data warehouse\n",
    "#### 1. Fact Table: Product Reviews\n",
    "![Product Reviews](diagrams/product_reviews.jpeg)\n",
    "#### 2. Dimension Table: Product\n",
    "![Product](diagrams/product.jpeg)\n",
    "#### 3. Dimension Table: Brand\n",
    "![Brand](diagrams/brand.jpeg)\n",
    "#### 4. Dimension Table: Reviewer\n",
    "![Reviewer](diagrams/reviewer.jpeg)\n",
    "#### 5. Dimension Table: Date\n",
    "![Date](diagrams/date.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL Completed using Python, Pandas and AWS Services : S3, RedShift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".final_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
